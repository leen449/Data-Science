{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSjG4e36xwZC"
   },
   "source": [
    "# Dataset Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHw0DibPuqUQ",
    "outputId": "8ac0da56-051d-4d67-87ef-ec4a5e85d130"
   },
   "outputs": [],
   "source": [
    "# Colab cell: install, configure, and test PRAW (run the whole cell)\n",
    "!pip install -q praw pandas\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Enter your credentials\n",
    "# -----------------------------\n",
    "# client_id: the short 14-char id shown under your app name on https://www.reddit.com/prefs/apps\n",
    "# client_secret: shown when you click \"edit\" on your app (keep it secret)\n",
    "# reddit_username: your reddit username (used in user_agent)\n",
    "\n",
    "client_id = \"hEXBI-j9iyrM_UsdLnmuAA\".strip()\n",
    "client_secret = getpass(\"Client secret (hidden input): \").strip()     # hidden input so you don't paste secret in notebook output\n",
    "reddit_username = \"Proof-Wolf-8881\".strip()\n",
    "\n",
    "# remove accidental invisible whitespace (tabs/newlines) that often cause 401 errors\n",
    "client_id = client_id.strip()\n",
    "client_secret = client_secret.strip()\n",
    "reddit_username = reddit_username.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Create a clear user_agent\n",
    "# -----------------------------\n",
    "# Make it descriptive (project name, version, and your reddit username)\n",
    "user_agent = f\"OffenseAgeProject/0.1 by u/{reddit_username}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Create the PRAW Reddit instance\n",
    "# -----------------------------\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Basic authorization check\n",
    "print(\"read-only status (should be True):\", reddit.read_only)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Small tests: fetch posts and comments\n",
    "# -----------------------------\n",
    "try:\n",
    "    print(\"\\nFetching 3 hot posts from r/AskReddit (titles):\")\n",
    "    for i, post in enumerate(reddit.subreddit(\"AskReddit\").hot(limit=3), start=1):\n",
    "        print(f\"{i}. {post.title[:200]} (id={post.id})\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    print(\"\\nFetching 5 recent comments from r/AskReddit:\")\n",
    "    for c in reddit.subreddit(\"AskReddit\").comments(limit=5):\n",
    "        author = c.author.name if c.author else \"[deleted]\"\n",
    "        created = datetime.utcfromtimestamp(c.created_utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        text_preview = c.body.replace(\"\\n\", \" \")[:200]\n",
    "        print(f\"- [{created}] {author}: {text_preview}\")\n",
    "except Exception as e:\n",
    "    print(\"Error while fetching data:\", type(e).__name__, str(e))\n",
    "    print(\"\\nIf you see a 401 error, check the troubleshooting steps below.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIywqpuovwxS",
    "outputId": "a7f52929-0844-4686-f0de-94a1c8375fd5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for sub in [\"gaming\", \"movies\", \"politics\"]:\n",
    "    print(f\"Collecting from r/{sub}...\")\n",
    "    batch = []\n",
    "    for comment in reddit.subreddit(sub).comments(limit=200):\n",
    "        if comment.author:\n",
    "            batch.append({\n",
    "                \"subreddit\": sub,\n",
    "                \"username\": str(comment.author),\n",
    "                \"account_age_days\": (datetime.utcnow() - datetime.utcfromtimestamp(comment.author.created_utc)).days,\n",
    "                \"comment_text\": comment.body\n",
    "            })\n",
    "        time.sleep(1)  # pause to avoid 429\n",
    "\n",
    "    # Save after each subreddit\n",
    "    df_batch = pd.DataFrame(batch)\n",
    "    df_batch.to_csv(f\"reddit_{sub}.csv\", index=False)\n",
    "    print(f\"Saved {len(df_batch)} comments from {sub}\")\n",
    "    all_data.extend(batch)\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(\"reddit_all.csv\", index=False)\n",
    "print(\"âœ… All done! Total:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daOLqZBg0IsW"
   },
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CmkSSgQ0GI3",
    "outputId": "58dea02c-66c5-41f4-f94f-24464004e020"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('reddit_all.csv')\n",
    "\n",
    "# Drop the 'username' column\n",
    "df.drop('username', axis=1, inplace=True)\n",
    "\n",
    "# Convert 'account_age_days' to 'account_age_years'\n",
    "df['account_age_years'] = (df['account_age_days'] / 365.25).round(2)\n",
    "\n",
    "# Drop the original 'account_age_days' column\n",
    "df.drop('account_age_days', axis=1, inplace=True)\n",
    "\n",
    "# Reorder the columns as requested\n",
    "df = df[['subreddit', 'account_age_years', 'comment_text']]\n",
    "\n",
    "# Print the first 5 rows of the modified DataFrame to verify the changes\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('modified_reddit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb7PeFhN0-gM"
   },
   "source": [
    "# Integrating using weak supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QCRRV8oZe3G",
    "outputId": "81abf3d7-1728-4980-b3d2-b4980d6c397d"
   },
   "outputs": [],
   "source": [
    "pip install snorkel pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8odznJpEZmnf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UAQRRzQZ5ra",
    "outputId": "34ee7d31-a12f-4d20-b44e-08eb2dc325b9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define labels as constants\n",
    "TOXIC = 1\n",
    "NOT_TOXIC = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "# Load the dataset\n",
    "# Make sure 'modified_reddit_data.csv' is in the same directory as your script\n",
    "df = pd.read_csv('modified_reddit_data.csv')\n",
    "\n",
    "# Preprocessing is crucial for weak supervision.\n",
    "# We'll fill any missing text values to prevent errors.\n",
    "df['comment_text'] = df['comment_text'].fillna('')\n",
    "\n",
    "## SECTION 1: DEFINE LABELING FUNCTIONS (LFs)\n",
    "\n",
    "# LF1: Check for explicit profanity\n",
    "# This is a strong signal for toxicity.\n",
    "@labeling_function()\n",
    "def check_profanity(x):\n",
    "    toxic_words = ['fuck', 'shit', 'asshole', 'bitch', 'damn', 'crap', 'bullshit']\n",
    "    return TOXIC if any(word in x.comment_text.lower() for word in toxic_words) else ABSTAIN\n",
    "\n",
    "# LF2: Look for aggressive or demeaning language\n",
    "# This targets a different type of toxicity.\n",
    "@labeling_function()\n",
    "def check_aggression(x):\n",
    "    aggressive_phrases = [\n",
    "        'you are so stupid',\n",
    "        'you are an idiot',\n",
    "        'you are an idiot',\n",
    "        'you guys',\n",
    "        'you son of a bitch'\n",
    "    ]\n",
    "    return TOXIC if any(phrase in x.comment_text.lower() for phrase in aggressive_phrases) else ABSTAIN\n",
    "\n",
    "# LF3: Check for very short, non-substantive comments that might be toxic.\n",
    "# Toxic comments are sometimes very short and lack detail.\n",
    "@labeling_function()\n",
    "def check_short_toxic_comment(x):\n",
    "    if len(x.comment_text) <= 5 and check_profanity(x) == ABSTAIN and x.comment_text.lower() not in ['k.', 'yes.', 'no.']:\n",
    "        return TOXIC\n",
    "    return ABSTAIN\n",
    "\n",
    "# LF4: Look for positive, gaming-related keywords.\n",
    "# This helps us identify likely non-toxic comments.\n",
    "@labeling_function()\n",
    "def check_positive_gaming_terms(x):\n",
    "    gaming_terms = ['great game', 'good times', 'fun', 'awesome', 'amazing', 'brilliant', 'loved it']\n",
    "    return NOT_TOXIC if any(term in x.comment_text.lower() for term in gaming_terms) else ABSTAIN\n",
    "\n",
    "# LF5: Check for question marks, which are often in non-toxic comments.\n",
    "@labeling_function()\n",
    "def check_questions(x):\n",
    "    return NOT_TOXIC if '?' in x.comment_text else ABSTAIN\n",
    "\n",
    "# LF6: Check for URLs, which are usually not toxic.\n",
    "@labeling_function()\n",
    "def check_url(x):\n",
    "    return NOT_TOXIC if 'http' in x.comment_text or 'www.' in x.comment_text else ABSTAIN\n",
    "\n",
    "## SECTION 2: APPLY THE LABELING FUNCTIONS\n",
    "\n",
    "# Group the labeling functions into a list\n",
    "lfs = [\n",
    "    check_profanity,\n",
    "    check_aggression,\n",
    "    check_short_toxic_comment,\n",
    "    check_positive_gaming_terms,\n",
    "    check_questions,\n",
    "    check_url\n",
    "]\n",
    "\n",
    "# Apply the LFs to the DataFrame\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df)\n",
    "\n",
    "## SECTION 3: ANALYZE AND TRAIN THE LABEL MODEL\n",
    "\n",
    "# Optional: Analyze the labeling function coverage, conflicts, and overlaps\n",
    "print(\"Labeling Function Analysis:\")\n",
    "print(LFAnalysis(L_train, lfs=lfs).lf_summary())\n",
    "\n",
    "# Train the Label Model to combine the noisy labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True) # cardinality=2 for TOXIC/NOT_TOXIC\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "# Get the denoised labels (the \"clean\" training set)\n",
    "df['toxic_label'] = label_model.predict(L=L_train)\n",
    "df['toxic_probability'] = label_model.predict_proba(L=L_train)[:, TOXIC]\n",
    "\n",
    "print(\"\\n--- Example of Denoised Labels ---\")\n",
    "print(df[['comment_text', 'toxic_label', 'toxic_probability']].head(20))\n",
    "\n",
    "## SECTION 4: USE LABELS TO TRAIN A FINAL CLASSIFIER\n",
    "\n",
    "# Now you have a high-quality training dataset with a 'toxic_label' column.\n",
    "# You can use this to train a more sophisticated model.\n",
    "\n",
    "# Example with a simple scikit-learn classifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Filter out comments where the LabelModel abstained (-1)\n",
    "df_clean = df[df['toxic_label'] != -1]\n",
    "\n",
    "if not df_clean.empty:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_clean['comment_text'],\n",
    "        df_clean['toxic_label'],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df_clean['toxic_label']\n",
    "    )\n",
    "\n",
    "    # Create a classification pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    # Train the final model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(\"\\n--- Final Model Performance (on a subset of data) ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo comments were labeled by the LabelModel. Please refine your labeling functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5ye_NDH3e1F"
   },
   "source": [
    "# The second approach using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_e4WjsndT6h",
    "outputId": "67d83ead-2cae-4343-88b1-58d88abf52e8"
   },
   "outputs": [],
   "source": [
    "pip install snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iQRAqlGe6UK"
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "# Define labels for clarity\n",
    "TOXIC = 1\n",
    "NOT_TOXIC = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "# Labeling Function 1: Check for a list of toxic words\n",
    "toxic_words = {'hate', 'racist', 'kill', 'stupid', 'idiot', 'moron', 'cringe'}\n",
    "@labeling_function()\n",
    "def lf_keywords_toxic(x):\n",
    "    return TOXIC if any(word in x.comment_text.lower() for word in toxic_words) else ABSTAIN\n",
    "\n",
    "# Labeling Function 2: Check for certain punctuation patterns that might indicate strong emotion\n",
    "@labeling_function()\n",
    "def lf_all_caps(x):\n",
    "    # A simple rule: if a comment is long and all in caps, it might be toxic\n",
    "    if len(x.comment_text) > 20 and x.comment_text.isupper():\n",
    "        return TOXIC\n",
    "    return ABSTAIN\n",
    "\n",
    "# Labeling Function 3: Check for specific positive sentiment words to label as \"not toxic\"\n",
    "positive_words = {'love', 'great', 'awesome', 'nice', 'good', 'happy'}\n",
    "@labeling_function()\n",
    "def lf_positive_keywords(x):\n",
    "    return NOT_TOXIC if any(word in x.comment_text.lower() for word in positive_words) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2s5LPDTOfK-3",
    "outputId": "2275266d-f7a7-4e36-abfa-7e117a060421"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "# Define the constants for the labels\n",
    "TOXIC = 1\n",
    "NOT_TOXIC = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "# Load the dataset (replace with your file name)\n",
    "df = pd.read_csv('reddit_all.csv')\n",
    "\n",
    "# Drop the 'username' column\n",
    "df.drop('username', axis=1, inplace=True)\n",
    "\n",
    "# Convert 'account_age_days' to 'account_age_years'\n",
    "df['account_age_years'] = (df['account_age_days'] / 365.25).round(2)\n",
    "\n",
    "# Drop the original 'account_age_days' column\n",
    "df.drop('account_age_days', axis=1, inplace=True)\n",
    "\n",
    "# Define the labeling functions\n",
    "toxic_words = {'hate', 'racist', 'stupid', 'idiot', 'moron'}\n",
    "@labeling_function()\n",
    "def lf_keywords_toxic(x):\n",
    "    return TOXIC if any(word in x.comment_text.lower() for word in toxic_words) else ABSTAIN\n",
    "\n",
    "positive_words = {'love', 'great', 'awesome', 'nice', 'good', 'happy'}\n",
    "@labeling_function()\n",
    "def lf_positive_keywords(x):\n",
    "    return NOT_TOXIC if any(word in x.comment_text.lower() for word in positive_words) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_all_caps(x):\n",
    "    if len(x.comment_text) > 20 and x.comment_text.isupper():\n",
    "        return TOXIC\n",
    "    return ABSTAIN\n",
    "\n",
    "# Collect all labeling functions\n",
    "lfs = [lf_keywords_toxic, lf_positive_keywords, lf_all_caps]\n",
    "\n",
    "# Apply the LFs to the data\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df)\n",
    "\n",
    "# Train the LabelModel\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "# Add the final 'toxicity' column to your dataframe\n",
    "df['toxicity'] = label_model.predict(L_train)\n",
    "\n",
    "# Display the first 10 rows with the new column\n",
    "print(df.head(10))\n",
    "\n",
    "# Save the final dataframe with the new column\n",
    "df.to_csv('labeled_reddit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvGPetZTrUYT"
   },
   "source": [
    "# The third approach using transformers piplines to proccess the comments with level of toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b1e71a29075746a98033ff3207215f49",
      "8c0d457cba6248688f70eef228b4260b",
      "00e464493d594ccca56dc3044ac70145",
      "580cf643b02b46e4815e3e33484a65e7",
      "70ae2c8092834f7bba3a2532e1558852",
      "cc4e4c499eb0478dacb594e400c6fe5e",
      "cb86b0342e9147f6998d8f8a305b91a0",
      "86413044680e43dc8e09a936427f3876",
      "ea06b27652fd4a049c04e662c5ae9e37",
      "39da686ab4a348d6b00fa7efada6f581",
      "5feb62e72d7f4af0803b7ea19bc6a9f9",
      "83d7875af7844f2aa781010bb5d6b442",
      "26c0f4d02bc64de0bd4eb4960763b90e",
      "70b512082198498ca31297f9b1e8bfe5",
      "b8738896b71443ef8a72153c83f67307",
      "ce50b72dbba94062a984aa6d3493ad8c",
      "3d061cc20dcc41cea025beb92856819b",
      "b85c09ee86e145b7aaed96eff14ba66a",
      "bc09561313294b5bb38e810d69a64cc5",
      "da4700fc267d43f49cbe55f402d394c6",
      "9584e5c1970e4ba2a3b1e36b62930f96",
      "a755e9fb2915457288d976d9aa7ce178",
      "838d354092084acaa026f6b4987dc6c2",
      "e4708522bcb74c1ba1880b5dac0f6d97",
      "630c99f1855b44d6981913dae208efe8",
      "add7c3606b7d40df9765f3db622f3d78",
      "6b2796024724426d929ac4a83f561156",
      "2f7b7c7315664daf8a6f597c8a48cd5a",
      "f4aaf03f061c42f9bab94a458c469e0e",
      "d707fd5eef4b47168782e3bafe33c691",
      "fa84d6ba32574070bb4e72d00fb581f8",
      "b0b0acceee0a47129292c6a5cc52c81a",
      "0b1a6a73fc2644c4b70deb8446d09f51",
      "9cefd049cc2b477da6a756ed0b0ee10d",
      "c8ad924387514c1686aef937e9c73a5e",
      "f70045b1910c4420b1feb75a86e01f4d",
      "94b5e5a3d64e4baea3c7bb6f885ecce1",
      "eb15f3670b9b435a8cd5828633640a8e",
      "4828da4c792f482abe63940cfcd4f2cc",
      "216d4ff9b9364f418dba0b51f78b55b4",
      "323a587434a74a2fbad436bd6f863677",
      "d5ad1748c46245cb873a3f9e875f6d80",
      "3d804e19a2e049bf9a10f41859c98557",
      "52dfaf276544409bb3310f00d98a3dd3",
      "d051a29a010640e4a5b98735d8e150eb",
      "42a8ff0a35ca47a68bb2e065d0c0f2ba",
      "afdbe2db251a4d60a077ea4c66f75c2f",
      "c6e3bc300ec144aea104449b694c01c9",
      "05217a286b764c309adfc8cb01e0c919",
      "74966037a0464b60a20a187e51be740b",
      "dea4080ae96c4df3883ee89dafd3a70a",
      "682935dd43d24f1cae6170c62f6ff530",
      "076213502f0b4cd9abdcf87e6d60498e",
      "616f3fd258b949d48baaf634fb04b496",
      "2826e02336c4411ab9a914de651e5739",
      "eee51ff4d98c42b79406e723291e4ee1",
      "994d0e5f6f9547fd8461ccc2ad2afd2a",
      "3bd3159ed1824e9585ab32ce345428f9",
      "17a7d83780c8456fa2b96ba3dea69ab1",
      "90639881213645c985cc4ce04dbc53f7",
      "4c1c6c0af08342bc8a196ccff9ea2f9d",
      "6e47dafc927d4419b17dc9c772cc19e3",
      "0f74cd2517b047e9a0d9c8aa5f483896",
      "e3dd4a060967421dabf4f483ff612640",
      "47314208b0ef4697b004284005d4729a",
      "097f6cd4ecdf4656818c5eb3285ca3c5"
     ]
    },
    "id": "kKyUSrT_mLxc",
    "outputId": "0fc52a6f-5875-46ae-802f-afa9144662ca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Define the file name for the input dataset.\n",
    "input_file = 'modified_reddit_data.csv'\n",
    "output_file = 'labeled_reddit_data.csv'\n",
    "\n",
    "# Load the modified dataset\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loading file: {input_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{input_file}' was not found. Please make sure it's uploaded to your Colab session.\")\n",
    "    exit()\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Load pre-trained toxicity classifier\n",
    "print(\"Loading pre-trained toxicity model...\")\n",
    "toxicity_classifier = pipeline(\"text-classification\",\n",
    "                               model=\"unitary/multilingual-toxic-xlm-roberta\",\n",
    "                               device=device,\n",
    "                               truncation=True)\n",
    "\n",
    "# Define a function to classify a single comment\n",
    "def classify_toxicity(text):\n",
    "    # Handle potential NaNs\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return {'label': 'not toxic', 'score': 1.0}\n",
    "\n",
    "    # The model expects a list of strings\n",
    "    result = toxicity_classifier([text])\n",
    "    return result[0]\n",
    "\n",
    "# Apply the classifier to the 'comment_text' column\n",
    "print(\"Applying toxicity classifier to comments...\")\n",
    "df['classification_result'] = df['comment_text'].apply(classify_toxicity)\n",
    "\n",
    "# Extract the label and score into new columns\n",
    "df['toxicity_label'] = df['classification_result'].apply(lambda x: x['label'])\n",
    "df['toxicity_score'] = df['classification_result'].apply(lambda x: x['score'])\n",
    "\n",
    "# Print unique labels and distribution before the final mapping\n",
    "print(\"\\nUnique labels output by the pre-trained model:\")\n",
    "print(df['toxicity_label'].unique())\n",
    "print(\"\\nDistribution of raw labels:\")\n",
    "print(df['toxicity_label'].value_counts())\n",
    "\n",
    "# --- THIS IS THE CRITICAL CHANGE ---\n",
    "# Use a confidence score threshold to classify as toxic or not.\n",
    "# We will use a threshold of 0.5.\n",
    "df['toxicity'] = df.apply(\n",
    "    lambda row: 1 if row['toxicity_label'] == 'toxic' and row['toxicity_score'] > 0.5 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop intermediate columns\n",
    "df.drop(['classification_result', 'toxicity_label', 'toxicity_score'], axis=1, inplace=True)\n",
    "\n",
    "# Print some results to verify\n",
    "print(\"\\nFirst 10 rows of the final dataframe:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nDistribution of final toxicity labels:\")\n",
    "print(df['toxicity'].value_counts())\n",
    "\n",
    "# Save the final dataframe to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"\\nData saved to 'labeled_reddit_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWkQFAuen3ot",
    "outputId": "becfd16f-3c99-44ab-fb77-09bb329e3ef8"
   },
   "outputs": [],
   "source": [
    "print(\"\\nDistribution of final toxicity labels:\")\n",
    "print(df['toxicity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfGU-EBtjf4j",
    "outputId": "4465abfe-5307-4123-86ad-1202273f4fa5"
   },
   "outputs": [],
   "source": [
    "# Group the data by subreddit and count the number of comments for each toxicity label.\n",
    "toxicity_per_subreddit = df.groupby('subreddit')['toxicity'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Optional: Add a total count per subreddit for context\n",
    "toxicity_per_subreddit['total_comments'] = toxicity_per_subreddit.sum(axis=1)\n",
    "\n",
    "print(\"\\nToxicity analysis per subreddit:\")\n",
    "print(toxicity_per_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NaZC6CH5lye"
   },
   "source": [
    "# Data insights visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBZHY0torePU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em-CNDvg5w5A"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('labeled_reddit_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhHP9tL258Eo",
    "outputId": "07ef69cb-4494-4a7b-f6aa-bcb51c8c3444"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nToxicity Distribution:\\n{df['toxicity'].value_counts()}\")\n",
    "print(f\"\\nSubreddit Distribution:\\n{df['subreddit'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4FDDhgOE5_9d",
    "outputId": "1a4af10c-7e34-45e0-f5c2-4624460ac1af"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(20, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "ln55O8xS6F06",
    "outputId": "5f3030ed-e306-4f84-cd32-d5c744600d85"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. How does the age of a user's account correlate with the likelihood of them posting a toxic comment?\n",
    "plt.subplot(2, 3, 1)\n",
    "toxic_ages = df[df['toxicity'] == 1]['account_age_years']\n",
    "non_toxic_ages = df[df['toxicity'] == 0]['account_age_years']\n",
    "\n",
    "plt.hist([non_toxic_ages, toxic_ages], bins=15, label=['Non-Toxic', 'Toxic'], alpha=0.7)\n",
    "plt.xlabel('Account Age (Years)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Account Age Distribution by Toxicity')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add summary statistics\n",
    "toxic_mean = toxic_ages.mean()\n",
    "non_toxic_mean = non_toxic_ages.mean()\n",
    "plt.axvline(toxic_mean, color='red', linestyle='--', alpha=0.8, label=f'Toxic Mean: {toxic_mean:.2f}')\n",
    "plt.axvline(non_toxic_mean, color='blue', linestyle='--', alpha=0.8, label=f'Non-Toxic Mean: {non_toxic_mean:.2f}')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "HHFlu3Ct6IFK",
    "outputId": "955b8963-22dd-4e93-89fa-088dd9a5668d"
   },
   "outputs": [],
   "source": [
    "# 2. Do certain Topics have a significantly higher proportion of toxic comments than others?\n",
    "plt.subplot(2, 3, 2)\n",
    "toxicity_by_subreddit = df.groupby('subreddit')['toxicity'].agg(['mean', 'count']).reset_index()\n",
    "toxicity_by_subreddit['toxicity_percentage'] = toxicity_by_subreddit['mean'] * 100\n",
    "\n",
    "bars = plt.bar(toxicity_by_subreddit['subreddit'], toxicity_by_subreddit['toxicity_percentage'],\n",
    "               color=['#ff9999', '#66b3ff', '#99ff99'])\n",
    "plt.xlabel('Subreddit')\n",
    "plt.ylabel('Toxicity Percentage (%)')\n",
    "plt.title('Toxicity Percentage by Subreddit')\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, percentage in zip(bars, toxicity_by_subreddit['toxicity_percentage']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "CvRONQTt6hBJ",
    "outputId": "c04f8bc2-4555-40cb-d5a2-649cbd93ae29"
   },
   "outputs": [],
   "source": [
    "# 3. Can we accurately predict the toxicity of a comment based on account age?\n",
    "plt.subplot(2, 3, 3)\n",
    "# Prepare data for logistic regression\n",
    "X = df[['account_age_years']]\n",
    "y = df['toxicity']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create prediction visualization\n",
    "x_range = np.linspace(df['account_age_years'].min(), df['account_age_years'].max(), 100).reshape(-1, 1)\n",
    "y_proba = model.predict_proba(x_range)[:, 1]\n",
    "\n",
    "plt.scatter(X_test, y_test, alpha=0.6, label='Actual Data')\n",
    "plt.plot(x_range, y_proba, color='red', linewidth=2, label='Prediction Probability')\n",
    "plt.xlabel('Account Age (Years)')\n",
    "plt.ylabel('Toxicity (0=Non-Toxic, 1=Toxic)')\n",
    "plt.title(f'Logistic Regression Prediction\\nAccuracy: {accuracy:.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mWvdNeR73Sc"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "This initial phase provided a clear overview of the Reddit comment dataset. The key finding is a significant class imbalance, with non-toxic comments greatly outnumbering toxic ones. Preliminary analysis also suggests variations in comment tone across the gaming, movies, and politics subreddits.\n",
    "\n",
    "These insights are critical for Phase 2, where the focus will shift to feature engineering and building a model that can effectively handle the imbalanced data to predict toxicity.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

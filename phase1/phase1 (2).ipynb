{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSjG4e36xwZC"
   },
   "source": [
    "# Dataset Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHw0DibPuqUQ",
    "outputId": "25537247-e736-43bf-e1ad-7e131be42898"
   },
   "outputs": [],
   "source": [
    "# Colab cell: install, configure, and test PRAW (run the whole cell)\n",
    "!pip install -q praw pandas\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Enter your credentials\n",
    "# -----------------------------\n",
    "# client_id: the short 14-char id shown under your app name on https://www.reddit.com/prefs/apps\n",
    "# client_secret: shown when you click \"edit\" on your app (keep it secret)\n",
    "# reddit_username: your reddit username (used in user_agent)\n",
    "\n",
    "client_id = \"hEXBI-j9iyrM_UsdLnmuAA\".strip()\n",
    "client_secret = getpass(\"Client secret (hidden input): \").strip()     # hidden input so you don't paste secret in notebook output\n",
    "reddit_username = \"Proof-Wolf-8881\".strip()\n",
    "\n",
    "# remove accidental invisible whitespace (tabs/newlines) that often cause 401 errors\n",
    "client_id = client_id.strip()\n",
    "client_secret = client_secret.strip()\n",
    "reddit_username = reddit_username.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Create a clear user_agent\n",
    "# -----------------------------\n",
    "# Make it descriptive (project name, version, and your reddit username)\n",
    "user_agent = f\"OffenseAgeProject/0.1 by u/{reddit_username}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Create the PRAW Reddit instance\n",
    "# -----------------------------\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Basic authorization check\n",
    "print(\"read-only status (should be True):\", reddit.read_only)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Small tests: fetch posts and comments\n",
    "# -----------------------------\n",
    "try:\n",
    "    print(\"\\nFetching 3 hot posts from r/AskReddit (titles):\")\n",
    "    for i, post in enumerate(reddit.subreddit(\"AskReddit\").hot(limit=3), start=1):\n",
    "        print(f\"{i}. {post.title[:200]} (id={post.id})\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    print(\"\\nFetching 5 recent comments from r/AskReddit:\")\n",
    "    for c in reddit.subreddit(\"AskReddit\").comments(limit=5):\n",
    "        author = c.author.name if c.author else \"[deleted]\"\n",
    "        created = datetime.utcfromtimestamp(c.created_utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        text_preview = c.body.replace(\"\\n\", \" \")[:200]\n",
    "        print(f\"- [{created}] {author}: {text_preview}\")\n",
    "except Exception as e:\n",
    "    print(\"Error while fetching data:\", type(e).__name__, str(e))\n",
    "    print(\"\\nIf you see a 401 error, check the troubleshooting steps below.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIywqpuovwxS",
    "outputId": "968cf465-357b-4027-e0db-b7dc93bd44cb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for sub in [\"gaming\", \"movies\", \"politics\"]:\n",
    "    print(f\"Collecting from r/{sub}...\")\n",
    "    batch = []\n",
    "    for comment in reddit.subreddit(sub).comments(limit=200):\n",
    "        if comment.author:\n",
    "            batch.append({\n",
    "                \"subreddit\": sub,\n",
    "                \"username\": str(comment.author),\n",
    "                \"account_age_days\": (datetime.utcnow() - datetime.utcfromtimestamp(comment.author.created_utc)).days,\n",
    "                \"comment_text\": comment.body\n",
    "            })\n",
    "        time.sleep(1)  # pause to avoid 429\n",
    "\n",
    "    # Save after each subreddit\n",
    "    df_batch = pd.DataFrame(batch)\n",
    "    df_batch.to_csv(f\"reddit_{sub}.csv\", index=False)\n",
    "    print(f\"Saved {len(df_batch)} comments from {sub}\")\n",
    "    all_data.extend(batch)\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(\"reddit_all.csv\", index=False)\n",
    "print(\"âœ… All done! Total:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daOLqZBg0IsW"
   },
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CmkSSgQ0GI3",
    "outputId": "724f420a-1235-40ea-cada-da0f4a13a11c"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('reddit_all.csv')\n",
    "\n",
    "# Drop the 'username' column\n",
    "df.drop('username', axis=1, inplace=True)\n",
    "\n",
    "# Convert 'account_age_days' to 'account_age_years'\n",
    "df['account_age_years'] = (df['account_age_days'] / 365.25).round(2)\n",
    "\n",
    "# Drop the original 'account_age_days' column\n",
    "df.drop('account_age_days', axis=1, inplace=True)\n",
    "\n",
    "# Reorder the columns as requested\n",
    "df = df[['subreddit', 'account_age_years', 'comment_text']]\n",
    "\n",
    "# Print the first 5 rows of the modified DataFrame to verify the changes\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('modified_reddit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb7PeFhN0-gM"
   },
   "source": [
    "# Integrating using weak supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QCRRV8oZe3G",
    "outputId": "8cf77f19-9617-4a9a-a44e-2323bd2b8ce9"
   },
   "outputs": [],
   "source": [
    "pip install snorkel pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8odznJpEZmnf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UAQRRzQZ5ra",
    "outputId": "6805f99b-8b98-4fef-fbec-282ea70e6446"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define labels as constants\n",
    "TOXIC = 1\n",
    "NOT_TOXIC = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "# Load the dataset\n",
    "# Make sure 'modified_reddit_data.csv' is in the same directory as your script\n",
    "df = pd.read_csv('modified_reddit_data.csv')\n",
    "\n",
    "# Preprocessing is crucial for weak supervision.\n",
    "# We'll fill any missing text values to prevent errors.\n",
    "df['comment_text'] = df['comment_text'].fillna('')\n",
    "\n",
    "## SECTION 1: DEFINE LABELING FUNCTIONS (LFs)\n",
    "\n",
    "# LF1: Check for explicit profanity\n",
    "# This is a strong signal for toxicity.\n",
    "@labeling_function()\n",
    "def check_profanity(x):\n",
    "    toxic_words = ['fuck', 'shit', 'asshole', 'bitch', 'damn', 'crap', 'bullshit']\n",
    "    return TOXIC if any(word in x.comment_text.lower() for word in toxic_words) else ABSTAIN\n",
    "\n",
    "# LF2: Look for aggressive or demeaning language\n",
    "# This targets a different type of toxicity.\n",
    "@labeling_function()\n",
    "def check_aggression(x):\n",
    "    aggressive_phrases = [\n",
    "        'you are so stupid',\n",
    "        'you are an idiot',\n",
    "        'you are an idiot',\n",
    "        'you guys',\n",
    "        'you son of a bitch'\n",
    "    ]\n",
    "    return TOXIC if any(phrase in x.comment_text.lower() for phrase in aggressive_phrases) else ABSTAIN\n",
    "\n",
    "# LF3: Check for very short, non-substantive comments that might be toxic.\n",
    "# Toxic comments are sometimes very short and lack detail.\n",
    "@labeling_function()\n",
    "def check_short_toxic_comment(x):\n",
    "    if len(x.comment_text) <= 5 and check_profanity(x) == ABSTAIN and x.comment_text.lower() not in ['k.', 'yes.', 'no.']:\n",
    "        return TOXIC\n",
    "    return ABSTAIN\n",
    "\n",
    "# LF4: Look for positive, gaming-related keywords.\n",
    "# This helps us identify likely non-toxic comments.\n",
    "@labeling_function()\n",
    "def check_positive_gaming_terms(x):\n",
    "    gaming_terms = ['great game', 'good times', 'fun', 'awesome', 'amazing', 'brilliant', 'loved it']\n",
    "    return NOT_TOXIC if any(term in x.comment_text.lower() for term in gaming_terms) else ABSTAIN\n",
    "\n",
    "# LF5: Check for question marks, which are often in non-toxic comments.\n",
    "@labeling_function()\n",
    "def check_questions(x):\n",
    "    return NOT_TOXIC if '?' in x.comment_text else ABSTAIN\n",
    "\n",
    "# LF6: Check for URLs, which are usually not toxic.\n",
    "@labeling_function()\n",
    "def check_url(x):\n",
    "    return NOT_TOXIC if 'http' in x.comment_text or 'www.' in x.comment_text else ABSTAIN\n",
    "\n",
    "## SECTION 2: APPLY THE LABELING FUNCTIONS\n",
    "\n",
    "# Group the labeling functions into a list\n",
    "lfs = [\n",
    "    check_profanity,\n",
    "    check_aggression,\n",
    "    check_short_toxic_comment,\n",
    "    check_positive_gaming_terms,\n",
    "    check_questions,\n",
    "    check_url\n",
    "]\n",
    "\n",
    "# Apply the LFs to the DataFrame\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df)\n",
    "\n",
    "## SECTION 3: ANALYZE AND TRAIN THE LABEL MODEL\n",
    "\n",
    "# Optional: Analyze the labeling function coverage, conflicts, and overlaps\n",
    "print(\"Labeling Function Analysis:\")\n",
    "print(LFAnalysis(L_train, lfs=lfs).lf_summary())\n",
    "\n",
    "# Train the Label Model to combine the noisy labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True) # cardinality=2 for TOXIC/NOT_TOXIC\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "# Get the denoised labels (the \"clean\" training set)\n",
    "df['toxic_label'] = label_model.predict(L=L_train)\n",
    "df['toxic_probability'] = label_model.predict_proba(L=L_train)[:, TOXIC]\n",
    "\n",
    "print(\"\\n--- Example of Denoised Labels ---\")\n",
    "print(df[['comment_text', 'toxic_label', 'toxic_probability']].head(20))\n",
    "\n",
    "## SECTION 4: USE LABELS TO TRAIN A FINAL CLASSIFIER\n",
    "\n",
    "# Now you have a high-quality training dataset with a 'toxic_label' column.\n",
    "# You can use this to train a more sophisticated model.\n",
    "\n",
    "# Example with a simple scikit-learn classifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Filter out comments where the LabelModel abstained (-1)\n",
    "df_clean = df[df['toxic_label'] != -1]\n",
    "\n",
    "if not df_clean.empty:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_clean['comment_text'],\n",
    "        df_clean['toxic_label'],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df_clean['toxic_label']\n",
    "    )\n",
    "\n",
    "    # Create a classification pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    # Train the final model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(\"\\n--- Final Model Performance (on a subset of data) ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo comments were labeled by the LabelModel. Please refine your labeling functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5ye_NDH3e1F"
   },
   "source": [
    "# The second approach using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_e4WjsndT6h",
    "outputId": "0ccc993d-7169-4633-a37d-4bda3f632d2a"
   },
   "outputs": [],
   "source": [
    "pip install snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iQRAqlGe6UK"
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "# Define labels for clarity\n",
    "TOXIC = 1\n",
    "NOT_TOXIC = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "# Labeling Function 1: Check for a list of toxic words\n",
    "toxic_words = {'hate', 'racist', 'kill', 'stupid', 'idiot', 'moron', 'cringe'}\n",
    "@labeling_function()\n",
    "def lf_keywords_toxic(x):\n",
    "    return TOXIC if any(word in x.comment_text.lower() for word in toxic_words) else ABSTAIN\n",
    "\n",
    "# Labeling Function 2: Check for certain punctuation patterns that might indicate strong emotion\n",
    "@labeling_function()\n",
    "def lf_all_caps(x):\n",
    "    # A simple rule: if a comment is long and all in caps, it might be toxic\n",
    "    if len(x.comment_text) > 20 and x.comment_text.isupper():\n",
    "        return TOXIC\n",
    "    return ABSTAIN\n",
    "\n",
    "# Labeling Function 3: Check for specific positive sentiment words to label as \"not toxic\"\n",
    "positive_words = {'love', 'great', 'awesome', 'nice', 'good', 'happy'}\n",
    "@labeling_function()\n",
    "def lf_positive_keywords(x):\n",
    "    return NOT_TOXIC if any(word in x.comment_text.lower() for word in positive_words) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2s5LPDTOfK-3",
    "outputId": "6237530c-16d2-4f33-ec6a-14cbb858964c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "# Define the constants for the labels\n",
    "TOXIC = 1\n",
    "NOT_TOXIC = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "# Load the dataset (replace with your file name)\n",
    "df = pd.read_csv('reddit_all.csv')\n",
    "\n",
    "# Drop the 'username' column\n",
    "df.drop('username', axis=1, inplace=True)\n",
    "\n",
    "# Convert 'account_age_days' to 'account_age_years'\n",
    "df['account_age_years'] = (df['account_age_days'] / 365.25).round(2)\n",
    "\n",
    "# Drop the original 'account_age_days' column\n",
    "df.drop('account_age_days', axis=1, inplace=True)\n",
    "\n",
    "# Define the labeling functions\n",
    "toxic_words = {'hate', 'racist', 'stupid', 'idiot', 'moron'}\n",
    "@labeling_function()\n",
    "def lf_keywords_toxic(x):\n",
    "    return TOXIC if any(word in x.comment_text.lower() for word in toxic_words) else ABSTAIN\n",
    "\n",
    "positive_words = {'love', 'great', 'awesome', 'nice', 'good', 'happy'}\n",
    "@labeling_function()\n",
    "def lf_positive_keywords(x):\n",
    "    return NOT_TOXIC if any(word in x.comment_text.lower() for word in positive_words) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_all_caps(x):\n",
    "    if len(x.comment_text) > 20 and x.comment_text.isupper():\n",
    "        return TOXIC\n",
    "    return ABSTAIN\n",
    "\n",
    "# Collect all labeling functions\n",
    "lfs = [lf_keywords_toxic, lf_positive_keywords, lf_all_caps]\n",
    "\n",
    "# Apply the LFs to the data\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df)\n",
    "\n",
    "# Train the LabelModel\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "# Add the final 'toxicity' column to your dataframe\n",
    "df['toxicity'] = label_model.predict(L_train)\n",
    "\n",
    "# Display the first 10 rows with the new column\n",
    "print(df.head(10))\n",
    "\n",
    "# Save the final dataframe with the new column\n",
    "df.to_csv('labeled_reddit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvGPetZTrUYT"
   },
   "source": [
    "# The third approach using transformers piplines to proccess the comments with level of toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "99acb48b915744da8386d45c62694620",
      "695b2d81f6d447aa966ef43d1938379b",
      "9a8bd78821c44301a53195fb8b773c52",
      "e03e076da8234dafb462fbfb54b05778",
      "ceb18b4d507a4a0596344162342392f4",
      "f343b6f249ef404db865edf4103ecdf8",
      "dcdcab13a3284d8eb4af155e2ae23c8a",
      "a86f1dcb43884008b8283867278c4d03",
      "00347d6b13e74f20b36f6fc3de018d0d",
      "937633f951f64ecb9e96e9d04c6d5ec9",
      "54c8384c30e6454ba282b8e594f13a0f",
      "6acaace4d4064f02a3c7724ae474914b",
      "e2cbbf39972a4b10aa0978efd6fabc6b",
      "4051a7ace3af4da196aaac029dae02bd",
      "e7285d67c65c473ba55807c5f0c45df8",
      "15935ab192294d0dbdf8b74463312433",
      "54765bd39b52433c88c1140fa37e6afa",
      "316b089df2454bb59f42a320dd6eb29b",
      "628761e2838a4e90ac97a5e84772bf1e",
      "3d1879dd48f041a4a494346964b55dcd",
      "b93aeeeca95a48fa8e405f17e5e2da99",
      "f3437180c5a147c98dd6e8d0d3741381",
      "41a6a67ea9684afa81c768b83d61da03",
      "53c4b9e66467452ca470792cdea021a7",
      "c8625eebcd4c492498b0ab8e55967b8b",
      "ae3175c4478c4956b471b8c3791766df",
      "97d4e7fedaa24393937f26da99b8eaaf",
      "f1cd6fc35816409e95c0fa3f73f6eadf",
      "1d0ea44d06e84cbea681e30061385707",
      "145f6a6267104470bcdf6cb3499a0c80",
      "c20755ad2c5d481f9660645987e574ac",
      "2cf2b3402231499695e1ed24499c3267",
      "93de1ac0a86d46bc9dbd258c991efd5d",
      "9d375b9e17f24382801d87111b426072",
      "bdf8e7db83834fc29ef9c1277ba51f88",
      "8ef823673ee142848d1f9bfd60a8c8f0",
      "2c1ed6c0ed8d4fd1ac98bf81cc256d66",
      "ced3519a6682467183db4fe60c45fa24",
      "5a6dfbd9dd8f4270aade4dc8ef18ca86",
      "9766bb87e9eb4387980bdb598d4a551a",
      "340e21f647fa42d5b48050cf45c12630",
      "3189b0e33b5d409fbedab1f0acf223d8",
      "7006d806db714fb98f509598761bf556",
      "a071d64b45584542848c1ab3a1f0eb80",
      "14640c35cefc47af924184a321a7a421",
      "6b282e816fbb47a08d479c1c75a08cf8",
      "09601fa198a342a98a28bea043edf6d7",
      "dd7b6ba301a14796953cc3d4db9ca3dd",
      "9d362c3746794926aa744a71778e75ad",
      "64325fa3a41546048e140f363df7af7c",
      "bdda7550d9d94e7a9cf9144535ed78cd",
      "e4ed08312a984a8685c8fa48a0172107",
      "b37254d5124047efae0fa075e4cde37c",
      "f5063a8e89f94a4f905b9bcc6dcc8f40",
      "bb2febe6d58c41e28dad24578c16ded4",
      "4f8d996267034ca3bc05c46967ddd4e8",
      "4aa80c018474424eae9cd24d5c02210e",
      "328e601b10284f86941d688c43a7054d",
      "1eda4397587f45bb8a854d5a97a6601d",
      "b4eddb098b244ef8b92b6175a5c4be4c",
      "99da7c062b4f463084dd8c04d500507a",
      "6d527e1d39624d27a52fb49110808bcc",
      "ccfff9760af24f7ab2d10f42025e58b4",
      "633d0df5432e4f7a89b6253f48ebe3d4",
      "66033558fa7e475aaa4606482b904fbb",
      "007ea57ab5e3472c86da4c3fe84a3a8a"
     ]
    },
    "id": "kKyUSrT_mLxc",
    "outputId": "ef91b428-5c27-40c9-9189-3c73fe793d4b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Define the file name for the input dataset.\n",
    "input_file = 'modified_reddit_data.csv'\n",
    "output_file = 'labeled_reddit_data.csv'\n",
    "\n",
    "# Load the modified dataset\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loading file: {input_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{input_file}' was not found. Please make sure it's uploaded to your Colab session.\")\n",
    "    exit()\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Load pre-trained toxicity classifier\n",
    "print(\"Loading pre-trained toxicity model...\")\n",
    "toxicity_classifier = pipeline(\"text-classification\",\n",
    "                               model=\"unitary/multilingual-toxic-xlm-roberta\",\n",
    "                               device=device,\n",
    "                               truncation=True)\n",
    "\n",
    "# Define a function to classify a single comment\n",
    "def classify_toxicity(text):\n",
    "    # Handle potential NaNs\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return {'label': 'not toxic', 'score': 1.0}\n",
    "\n",
    "    # The model expects a list of strings\n",
    "    result = toxicity_classifier([text])\n",
    "    return result[0]\n",
    "\n",
    "# Apply the classifier to the 'comment_text' column\n",
    "print(\"Applying toxicity classifier to comments...\")\n",
    "df['classification_result'] = df['comment_text'].apply(classify_toxicity)\n",
    "\n",
    "# Extract the label and score into new columns\n",
    "df['toxicity_label'] = df['classification_result'].apply(lambda x: x['label'])\n",
    "df['toxicity_score'] = df['classification_result'].apply(lambda x: x['score'])\n",
    "\n",
    "# Print unique labels and distribution before the final mapping\n",
    "print(\"\\nUnique labels output by the pre-trained model:\")\n",
    "print(df['toxicity_label'].unique())\n",
    "print(\"\\nDistribution of raw labels:\")\n",
    "print(df['toxicity_label'].value_counts())\n",
    "\n",
    "# --- THIS IS THE CRITICAL CHANGE ---\n",
    "# Use a confidence score threshold to classify as toxic or not.\n",
    "# We will use a threshold of 0.5.\n",
    "df['toxicity'] = df.apply(\n",
    "    lambda row: 1 if row['toxicity_label'] == 'toxic' and row['toxicity_score'] > 0.5 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop intermediate columns\n",
    "df.drop(['classification_result', 'toxicity_label', 'toxicity_score'], axis=1, inplace=True)\n",
    "\n",
    "# Print some results to verify\n",
    "print(\"\\nFirst 10 rows of the final dataframe:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nDistribution of final toxicity labels:\")\n",
    "print(df['toxicity'].value_counts())\n",
    "\n",
    "# Save the final dataframe to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"\\nData saved to 'labeled_reddit_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWkQFAuen3ot",
    "outputId": "b5390bcd-901b-4a5e-f833-fee366d1bbdc"
   },
   "outputs": [],
   "source": [
    "print(\"\\nDistribution of final toxicity labels:\")\n",
    "print(df['toxicity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfGU-EBtjf4j",
    "outputId": "cbd8b133-dca4-42a6-8fc9-9df761262d9b"
   },
   "outputs": [],
   "source": [
    "# Group the data by subreddit and count the number of comments for each toxicity label.\n",
    "toxicity_per_subreddit = df.groupby('subreddit')['toxicity'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Optional: Add a total count per subreddit for context\n",
    "toxicity_per_subreddit['total_comments'] = toxicity_per_subreddit.sum(axis=1)\n",
    "\n",
    "print(\"\\nToxicity analysis per subreddit:\")\n",
    "print(toxicity_per_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NaZC6CH5lye"
   },
   "source": [
    "# Data insights visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBZHY0torePU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em-CNDvg5w5A"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('labeled_reddit_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhHP9tL258Eo",
    "outputId": "3cbb69ca-1623-4579-b827-e894b871e5cb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nToxicity Distribution:\\n{df['toxicity'].value_counts()}\")\n",
    "print(f\"\\nSubreddit Distribution:\\n{df['subreddit'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4FDDhgOE5_9d",
    "outputId": "c7557796-82ec-4a1e-e4a4-3fa217487143"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(20, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "ln55O8xS6F06",
    "outputId": "b1223b77-a094-4a9f-a773-d4bf5b9481ae"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. How does the age of a user's account correlate with the likelihood of them posting a toxic comment?\n",
    "plt.subplot(2, 3, 1)\n",
    "toxic_ages = df[df['toxicity'] == 1]['account_age_years']\n",
    "non_toxic_ages = df[df['toxicity'] == 0]['account_age_years']\n",
    "\n",
    "plt.hist([non_toxic_ages, toxic_ages], bins=15, label=['Non-Toxic', 'Toxic'], alpha=0.7)\n",
    "plt.xlabel('Account Age (Years)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Account Age Distribution by Toxicity')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add summary statistics\n",
    "toxic_mean = toxic_ages.mean()\n",
    "non_toxic_mean = non_toxic_ages.mean()\n",
    "plt.axvline(toxic_mean, color='red', linestyle='--', alpha=0.8, label=f'Toxic Mean: {toxic_mean:.2f}')\n",
    "plt.axvline(non_toxic_mean, color='blue', linestyle='--', alpha=0.8, label=f'Non-Toxic Mean: {non_toxic_mean:.2f}')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "HHFlu3Ct6IFK",
    "outputId": "929aef2a-d8fd-4db6-dfad-e3ec0c1867c7"
   },
   "outputs": [],
   "source": [
    "# 2. Do certain Topics have a significantly higher proportion of toxic comments than others?\n",
    "plt.subplot(2, 3, 2)\n",
    "toxicity_by_subreddit = df.groupby('subreddit')['toxicity'].agg(['mean', 'count']).reset_index()\n",
    "toxicity_by_subreddit['toxicity_percentage'] = toxicity_by_subreddit['mean'] * 100\n",
    "\n",
    "bars = plt.bar(toxicity_by_subreddit['subreddit'], toxicity_by_subreddit['toxicity_percentage'],\n",
    "               color=['#ff9999', '#66b3ff', '#99ff99'])\n",
    "plt.xlabel('Subreddit')\n",
    "plt.ylabel('Toxicity Percentage (%)')\n",
    "plt.title('Toxicity Percentage by Subreddit')\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, percentage in zip(bars, toxicity_by_subreddit['toxicity_percentage']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "CvRONQTt6hBJ",
    "outputId": "8f259411-cf74-4ed2-81e2-62d5708a65ea"
   },
   "outputs": [],
   "source": [
    "# 3. Can we accurately predict the toxicity of a comment based on account age?\n",
    "plt.subplot(2, 3, 3)\n",
    "# Prepare data for logistic regression\n",
    "X = df[['account_age_years']]\n",
    "y = df['toxicity']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create prediction visualization\n",
    "x_range = np.linspace(df['account_age_years'].min(), df['account_age_years'].max(), 100).reshape(-1, 1)\n",
    "y_proba = model.predict_proba(x_range)[:, 1]\n",
    "\n",
    "plt.scatter(X_test, y_test, alpha=0.6, label='Actual Data')\n",
    "plt.plot(x_range, y_proba, color='red', linewidth=2, label='Prediction Probability')\n",
    "plt.xlabel('Account Age (Years)')\n",
    "plt.ylabel('Toxicity (0=Non-Toxic, 1=Toxic)')\n",
    "plt.title(f'Logistic Regression Prediction\\nAccuracy: {accuracy:.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OikRa8J9nAym",
    "outputId": "9b394304-9f35-4718-d9c8-487b84f2c416"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('labeled_reddit_data.csv')\n",
    "\n",
    "# --- 1. Overview ---\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(df['toxicity'].value_counts())\n",
    "\n",
    "# --- 2. Toxicity rates per subreddit ---\n",
    "toxicity_rates = df.groupby('subreddit')['toxicity'].mean() * 100\n",
    "print(\"\\nToxicity rates (%):\")\n",
    "print(toxicity_rates)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=toxicity_rates.index, y=toxicity_rates.values, palette='viridis')\n",
    "plt.ylabel(\"Toxicity Rate (%)\")\n",
    "plt.title(\"Toxicity Rate per Subreddit\")\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Account age correlation ---\n",
    "correlation = df['account_age_years'].corr(df['toxicity'])\n",
    "print(\"\\nCorrelation between account age and toxicity:\", correlation)\n",
    "\n",
    "# Boxplot: account age vs toxicity\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='toxicity', y='account_age_years', data=df)\n",
    "plt.xlabel(\"Toxicity (0=Non-toxic, 1=Toxic)\")\n",
    "plt.ylabel(\"Account Age (Years)\")\n",
    "plt.title(\"Account Age vs Toxicity\")\n",
    "plt.show()\n",
    "\n",
    "# --- 4. Predictive modeling using only account age ---\n",
    "# Features and target\n",
    "X = df[['account_age_years']]\n",
    "y = df['toxicity']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nLogistic Regression Accuracy (account age only):\", round(accuracy*100,2), \"%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
